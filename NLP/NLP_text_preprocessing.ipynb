{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNXv+nfKMb3bZvY9sXTBceY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Suraez/tensorflow-hands_on/blob/main/NLP/NLP_text_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOGt-XzhPT98"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "URL = 'https://www.gutenberg.org/cache/epub/69813/pg69813.txt'\n",
        "response = requests.get(URL)\n",
        "data = response.text\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aas0cE5mPj6S",
        "outputId": "9976d059-9690-4184-dfea-84c91ca2647b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "310634"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "text = re.sub(r'[^\\w\\s]', '', data)\n",
        "text"
      ],
      "metadata": {
        "id": "OihWxuUvP1UN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text.lower()\n",
        "index = text.find('Snow')\n",
        "print(index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIvYapStQlCM",
        "outputId": "eb3fc06b-c522-4152-a6a1-1af4ed0ddffc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1894\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = text[1894:]"
      ],
      "metadata": {
        "id": "EuC71UZRRNY0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text)"
      ],
      "metadata": {
        "id": "7A-blJE7RQVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "8-4QOJ7NRyQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "tokens = nltk.word_tokenize(text)"
      ],
      "metadata": {
        "id": "NKqZ2PaIRn59"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BAG OF WORDS (BOW)"
      ],
      "metadata": {
        "id": "3lzEfMoUSv1T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BOW is the second step in text preprocessing where we convert text to vectors or tensors. In this process, a voabulary is created first from a corpus and then can be used to create vectors for sentences."
      ],
      "metadata": {
        "id": "i2-d5W9gWhU6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####BOW using sklearn"
      ],
      "metadata": {
        "id": "VVew5F6GW2fR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sklearn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_JGfu1OTgp3",
        "outputId": "f75fc112-18c9-4e71-8cbc-a60bb41d6875"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.post1.tar.gz (3.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: sklearn\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sklearn: filename=sklearn-0.0.post1-py3-none-any.whl size=2344 sha256=c1291c43faa5436e3aad43767ed676d68d81e1bbd93c0ee4da9c32d54bde1158\n",
            "  Stored in directory: /root/.cache/pip/wheels/14/25/f7/1cc0956978ae479e75140219088deb7a36f60459df242b1a72\n",
            "Successfully built sklearn\n",
            "Installing collected packages: sklearn\n",
            "Successfully installed sklearn-0.0.post1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = 'He is a good boy. She is a good girl. Boy and girl are good.' #having only one document"
      ],
      "metadata": {
        "id": "p8-MBYz-XXlH"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to tokenize first to creat a vocab from the corpus. You can either use either nltk or spacy. However spacy provides more options like entities, lemma etc. "
      ],
      "metadata": {
        "id": "4FCN7uWZXgaL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using nltk"
      ],
      "metadata": {
        "id": "K8rqIVwFYfQM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "import nltk\n",
        "tokens = nltk.word_tokenize(corpus)\n",
        "for token in tokens:\n",
        "  print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HDc2mWkXni0",
        "outputId": "ec1de727-164f-4b11-81c0-a6aa24792327"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "He\n",
            "is\n",
            "a\n",
            "good\n",
            "boy\n",
            ".\n",
            "She\n",
            "is\n",
            "a\n",
            "good\n",
            "girl\n",
            ".\n",
            "Boy\n",
            "and\n",
            "girl\n",
            "are\n",
            "good\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using spacy"
      ],
      "metadata": {
        "id": "3_obqIYaYt8_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(corpus) # i didn't call it tokens becuase spacy provides a lot more \n",
        "for token in doc: # than just tokens although output is same\n",
        "  print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgP5xsykX48N",
        "outputId": "2dde6a38-2c74-4efd-e14f-bfafc50fe8c8"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "He\n",
            "is\n",
            "a\n",
            "good\n",
            "boy\n",
            ".\n",
            "She\n",
            "is\n",
            "a\n",
            "good\n",
            "girl\n",
            ".\n",
            "Boy\n",
            "and\n",
            "girl\n",
            "are\n",
            "good\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have tokens from the corpus we can create vocabulary"
      ],
      "metadata": {
        "id": "X5b-V2hMZQvh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectorizer.fit(tokens) # creating the vocabulary\n",
        "print(vectorizer.vocabulary_) # accessing the vocabulary\n",
        "print(vectorizer.idf_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCk_-ftVR2V2",
        "outputId": "1210cf32-7b10-4ff7-f5ae-beb172ba0fb9"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'he': 5, 'is': 6, 'good': 4, 'boy': 2, 'she': 7, 'girl': 3, 'and': 0, 'are': 1}\n",
            "[3.2512918  3.2512918  2.84582669 2.84582669 2.55814462 3.2512918\n",
            " 2.84582669 3.2512918 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "let' create the vector from a sentence"
      ],
      "metadata": {
        "id": "aM_dLsMQes-d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentence = 'and are boy'\n",
        "vector = vectorizer.transform([test_sentence])\n",
        "print(vector.shape)\n",
        "print(vector.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4OWdSmBONqA",
        "outputId": "4e48c006-57f0-4c41-ffe8-48ef858d7acb"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 8)\n",
            "[[0.60126144 0.60126144 0.52627878 0.         0.         0.\n",
            "  0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the shape of the vector is (1, 8) cause there are 8 elements in the vocabulary\n",
        "and in the second output, the first, second and third elements of array are non-zero cause at first index , second index and third index in vocabulary lies the 'and' , 'are' and 'boy' respectively . see the cell output above this cell where the vocabulary is accessed and index is strored in value of the word"
      ],
      "metadata": {
        "id": "OYusE_4Md0Il"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### BOW using Keras"
      ],
      "metadata": {
        "id": "VlENzb7cfC-0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "let's use the same corpus and tokens created in BOW using spacy and test_sentence"
      ],
      "metadata": {
        "id": "MMTdb999fcY4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "LuMmWTaBV--B",
        "outputId": "c8f7885d-4d0d-4d19-b10d-4f8a811c55fc"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'He is a good boy. She is a good girl. Boy and girl are good.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfFe72-LfoGW",
        "outputId": "264e2c9f-2161-438f-a4b5-e0bc0afaedf4"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['He',\n",
              " 'is',\n",
              " 'a',\n",
              " 'good',\n",
              " 'boy',\n",
              " '.',\n",
              " 'She',\n",
              " 'is',\n",
              " 'a',\n",
              " 'good',\n",
              " 'girl',\n",
              " '.',\n",
              " 'Boy',\n",
              " 'and',\n",
              " 'girl',\n",
              " 'are',\n",
              " 'good',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(tokens) # creating the vocabulary\n",
        "print(tokenizer.word_index) # accessing the vocabulary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVoIK7VofpQw",
        "outputId": "f1e82a43-efdc-467e-dd95-815d3b8f47ee"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'good': 1, 'is': 2, 'a': 3, 'boy': 4, 'girl': 5, 'he': 6, 'she': 7, 'and': 8, 'are': 9}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentence = 'boy and girl'"
      ],
      "metadata": {
        "id": "wStoiGH-hBSu"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector = tokenizer.texts_to_matrix(test_sentence)\n",
        "vector"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJwewD8xf7FT",
        "outputId": "ef4c4647-d674-461a-fc2a-11ded87313b7"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember there are different approaches for BOW. keras provides the most simple one whereas spacy provides the TF-IDF approach. The most popular approach is TF-IDF approach."
      ],
      "metadata": {
        "id": "i3yDiGxIh5X2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CSW5_MhSgIYm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}